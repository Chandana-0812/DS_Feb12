{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAR7zG/VYd2YDke0s2vWn6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chandana-0812/DS_Feb12/blob/main/ANN_Basic_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMDr-f8lPIMK",
        "outputId": "cb52281e-c78b-43c1-9300-64262e8ed0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of observations: 791\n",
            "Total number of inputs/features: 13\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sklearn.preprocessing as preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 1: Read Excel file\n",
        "df = pd.read_excel('Final_Data_Absorption_21.xlsx')\n",
        "\n",
        "\n",
        "# Select the first 37 columns\n",
        "A = df.iloc[:, 0:16]\n",
        "\n",
        "# Get the dimensions of A\n",
        "na1, na2 = A.shape\n",
        "\n",
        "# Descriptor Matrix for catalysts D1(1,5): ATOMIC NUMBERS\n",
        "D1 = np.array([26, 40, 25, 28, 58, 39, 27, 22])\n",
        "\n",
        "# Assigning of values from A to A1 matrix\n",
        "A1 = np.zeros((na1, 13))  # total inputs\n",
        "A1[:,7:13] = A.iloc[:,9:15]  # Operating param\n",
        "target1 = A.iloc[:, 15]  # H2\n",
        "# target1=A(:,31);  %CO\n",
        "# target1=A(:,32); %CO2\n",
        "# target1=A(:,33); %CH4\n",
        "\n",
        "# METAL1\n",
        "for i in range(na1):\n",
        "    count = 0\n",
        "    j1 = []\n",
        "    for j in range(8):\n",
        "        if A.iloc[i, j] != 0:\n",
        "            count += 1\n",
        "            j1.append(j)\n",
        "    for j2 in range(count):\n",
        "        A1[i, (2 * j2)] = D1[j1[j2]]  # Active metal\n",
        "        A1[i, (2 * j2 + 1)] = A.iloc[i, j1[j2]]  # Active metal comp\n",
        "        for j2 in range(3):\n",
        "            if A1[i, (2 * j2)] == 0:  # Catalyst Composition if zero\n",
        "                A1[i, (2 * j2)] = np.min(D1)\n",
        "                A1[i, (2 * j2 + 1)] = 0\n",
        "\n",
        "for i in range(na1):\n",
        "    for j in range(9):\n",
        "        if A.iloc[i, j] == 100:\n",
        "            for j2 in range(3):\n",
        "                A1[i, (2 * j2)] = np.min(D1)  # No Catalyst\n",
        "                A1[i, (2 * j2 + 1)] = 0  # Its Composition\n",
        "\n",
        "na1, na3 = A1.shape\n",
        "print(f'Total number of observations: {na1}')\n",
        "print(f'Total number of inputs/features: {na3}')\n",
        "null_indices = np.isnan(A1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "A1 = pd.DataFrame(A1)\n",
        "# Load your data into InputData and TargetData\n",
        "InputData = A1.iloc[:, 0:13]\n",
        "TargetData = A.iloc[:, 15]\n",
        "\n",
        "# Transpose the input and target data\n",
        "x = InputData\n",
        "t = TargetData\n",
        "\n",
        "# Choose a Training Function\n",
        "trainFcn = 'adam'  # Adam\n",
        "np.random.seed(42)\n",
        "# Create a Fitting Network\n",
        "hiddenLayerSize = (5,)  # 4 nodes in one hidden layer\n",
        "net = MLPRegressor(hidden_layer_sizes=hiddenLayerSize, activation='tanh', solver=trainFcn,\n",
        "                   max_iter=5000, learning_rate_init=0.005, momentum=0.1, alpha=1e-10)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "x_train, x_test, t_train, t_test = train_test_split(x, t, test_size=0.30, random_state=1)\n",
        "# Min & Max values for inputs and outputs\n",
        "mnx_train= np.min(x_train, axis=0)\n",
        "mxx_train= np.max(x_train, axis=0)\n",
        "\n",
        "print('mnx_train',mnx_train)\n",
        "print('mxx_train',mxx_train)\n",
        "\n",
        "mnx_test=np.min(x_test, axis=0)\n",
        "mxx_test=np.max(x_test, axis=0)\n",
        "\n",
        "mnt_train=min(t_train)\n",
        "mxt_train=max(t_train)\n",
        "\n",
        "mnt_test=min(t_test)\n",
        "mxt_test=max(t_test)\n",
        "\n",
        "na1_tr = A1.shape[0]  # Assuming na1_tr is the number of rows in B1\n",
        "na3_tr = A1.shape[1]  # Assuming na3_tr is the number of columns in B1\n",
        "\n",
        "# Input scaling\n",
        "X_train = ((x_train - mnx_train) / (mxx_train - mnx_train)) * 10.0 - 5.0\n",
        "X_test= ((x_test - mnx_test) / (mxx_test - mnx_test)) * 10.0 - 5.0\n",
        "\n",
        "# Normalize Output Values\n",
        "t_train=((t_train - mnt_train)/(mxt_train - mnt_train))*2-1\n",
        "t_test=((t_test - mnt_test)/(mxt_test - mnt_test))*2-1\n",
        "\n",
        "# Normalize data\n",
        "#sc = MinMaxScaler()\n",
        "#X_train = sc.fit_transform(x_train)\n",
        "#X_test = sc.transform(x_test)\n",
        "\n",
        "# Train the network\n",
        "net.fit(X_train, t_train)\n",
        "\n",
        "# Make predictions on training and testing data\n",
        "y_train = net.predict(X_train)\n",
        "y_test = net.predict(X_test)\n",
        "\n",
        "# Denormalize output data\n",
        "y_train = ((mnt_train + mxt_train) + y_train*(mxt_train - mnt_train))/2\n",
        "y_test = ((mnt_test + mxt_test) + y_test*(mxt_test - mnt_test))/2\n",
        "\n",
        "t_train=((mnt_train + mxt_train) + t_train*(mxt_train - mnt_train))/2\n",
        "t_test=((mnt_test + mxt_test) + t_test*(mxt_test - mnt_test))/2\n",
        "\n",
        "# Calculate performance measures for training data\n",
        "RMSE_train = np.sqrt(mean_squared_error(t_train, y_train))\n",
        "Rsq_train = r2_score(t_train, y_train)\n",
        "AAD_train = np.mean(np.abs(y_train - t_train))\n",
        "\n",
        "# Calculate performance measures for testing data\n",
        "RMSE_test = np.sqrt(mean_squared_error(t_test, y_test))\n",
        "Rsq_test = r2_score(t_test, y_test)\n",
        "AAD_test = np.mean(np.abs(y_test - t_test))\n",
        "\n",
        "# Print performance measures\n",
        "print(\"Performance measures for training data:\")\n",
        "print(\"RMSE:\", RMSE_train)\n",
        "print(\"R-squared:\", Rsq_train)\n",
        "print(\"AAD:\", AAD_train)\n",
        "print(\"\\nPerformance measures for testing data:\")\n",
        "print(\"RMSE:\", RMSE_test)\n",
        "print(\"R-squared:\", Rsq_test)\n",
        "print(\"AAD:\", AAD_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "WAZpiKIJe0vE",
        "outputId": "91dcf59c-fb02-47a0-e7f7-48c99147ec24"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mnx_train 0      26.0\n",
            "1       0.6\n",
            "2      22.0\n",
            "3       0.0\n",
            "4      22.0\n",
            "5       0.0\n",
            "6       0.0\n",
            "7       9.0\n",
            "8       5.0\n",
            "9       2.0\n",
            "10      0.0\n",
            "11    100.0\n",
            "12     12.0\n",
            "dtype: float64\n",
            "mxx_train 0        58.000000\n",
            "1        10.000000\n",
            "2        58.000000\n",
            "3        11.400000\n",
            "4        39.000000\n",
            "5         4.600000\n",
            "6         0.000000\n",
            "7        50.000000\n",
            "8     50000.000000\n",
            "9        60.000000\n",
            "10      144.189526\n",
            "11      350.000000\n",
            "12       40.000000\n",
            "dtype: float64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nMLPRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-0a281d209566>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Train the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m# Make predictions on training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    435\u001b[0m         )\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfirst_pass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, y, incremental, reset)\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1613\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m   1614\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         )\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1107\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nMLPRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicting result for Single Observation\n",
        "xnew=[26, 2.5,\t28,\t5,\t22,\t2.5,\t90,\t45,\t37,\t8, 5,\t250,\t12]\n",
        "print(net.predict(sc.transform([xnew])))"
      ],
      "metadata": {
        "id": "8vfDbeZvf4ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained weights\n",
        "#weights = net.coefs_\n",
        "# Extract weights\n",
        "input_hidden_weights = net.coefs_[0]  # Weights between input and hidden layer\n",
        "hidden_output_weights = net.coefs_[1]  # Weights between hidden and output layer\n",
        "\n",
        "# Print weights\n",
        "print(\"Input to Hidden Weights:\")\n",
        "print(input_hidden_weights)\n",
        "print(\"\\nHidden to Output Weights:\")\n",
        "print(hidden_output_weights)\n",
        "\n",
        "# Print Bias terms\n",
        "# Extract biases\n",
        "hidden_biases = net.intercepts_[0]  # Biases for the hidden layer\n",
        "output_bias = net.intercepts_[1]    # Bias for the output layer\n",
        "\n",
        "print(\"Input to Hidden Bias:\")\n",
        "print(hidden_biases)\n",
        "print(\"\\nHidden to Output Bias:\")\n",
        "print(output_bias)\n",
        "\n",
        "# Print or further process the weights\n",
        "#print(weights)"
      ],
      "metadata": {
        "id": "ajTuypenisvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%whos\n"
      ],
      "metadata": {
        "id": "USvcrv887LZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}